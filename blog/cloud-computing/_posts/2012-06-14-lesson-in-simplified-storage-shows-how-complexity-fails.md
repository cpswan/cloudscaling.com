---
title: 'Lesson in simplified storage shows how complexity fails'
link: http://cloudscaling.com/blog/cloud-computing/lesson-in-simplified-storage-shows-how-complexity-fails/
author: Randy Bias
description: 
post_id: 4012
created: 2012/06/14 08:57:33
created_gmt: 2012/06/14 15:57:33
comment_status: open
post_name: lesson-in-simplified-storage-shows-how-complexity-fails
status: publish
post_type: post
layout: post
category: cloud-computing
---

# Lesson in simplified storage shows how complexity fails

_(This post originally appeared at [O'Reilly Radar](http://radar.oreilly.com/2012/04/complexity-vs-simplicity.html).)_ The default approach to most complex problems is to engineer a complex solution. We see this in IT, generally, and in cloud computing specifically. Experience has taught us, however, that large-scale systems belie this tendency: Simpler solutions are best for solving complex problems. When developers write code, they talk about "elegant code," meaning they are able to come up with a concise, simple, solution to a complex coding problem. In this article, I hope to provide further clarity around what I mean by simple solutions and how they differ from more complex ones. **Simple vs. complex** Understanding simplicity seems ... well, simple. Simple systems have fewer parts. Think of a nail. It's usually a single piece of steel with one end flattened, and it does just one thing, so not much can go wrong. A hammer is a slightly more complex, yet still simple, tool. It might be made of a couple of parts, but it really has one function and not much can go wrong. In comparison to these, a power drill is a significantly more complex tool and computers are far, far, more complex. As parts increase and as the number of functions that are provided by a system increases, the complexity increases. Related to this phenomena is the simplicity or complexity of the parts themselves. Simpler parts can be more easily assembled into more complex systems that are more reliable because their parts are simpler. Whereas putting together complex systems from complex parts leads us toward building fragile and brittle Rube Goldberg contraptions. Complexity kills scalable systems. If you want higher uptime and reliability in your IT system, you want smaller, simpler systems that fail less often because they have simpler, fewer parts. **An example: storage** A system that is twice as complex as another system isn't just twice as likely to fail, it's four times as likely to fail. In order to illustrate this and to drive home some points, I'm going to compare direct-attached storage (DAS) and storage-area network (SAN) technologies. DAS is a very simple approach to IT storage. It has fewer features than SAN, but it can also fail in fewer ways. In the cloud computing space, some feel that one of Amazon Web Services' (AWS) weaknesses is that it provides only DAS by default. To counter this, many competitors run SAN-based cloud services only, taking on the complexity of SAN-based storage as a bid for differentiation. Yet AWS remains the leader in every regard in cloud computing, mainly because it sticks to a principle of simplicity. If we look at DAS versus SAN and trace the path data takes when written by an application running "in the cloud," it would look something like [this figure](http://cdn.oreilly.com/radar/images/posts/0412-data-path-san-vs-das-lg.png): 

![DAS vs San data path](http://cdn.oreilly.com/radar/images/posts/0412-data-path-san-vs-das-450.png) _DAS vs. San data path. [Click to enlarge](http://cdn.oreilly.com/radar/images/posts/0412-data-path-san-vs-das-lg.png)._

(A quick aside: Please note that I have left out all kinds of components, such as disk drive firmware, RAID controller firmware, complexities in networking/switching, and the like. All of these would count here as components.) A piece of data written by the application running inside of the guest operating system (OS) of a virtual server flows as follows with DAS in place: 1\. The guest OS filesystem (FS) software accepts a request to write a block of data. 2\. The guest OS writes it to its "disk," which is actually a virtual disk drive using the "block stack" (BS) [1] in its kernel. 3\. The guest OS has a para-virtualization (PV) disk driver [2] that knows how to write the block of data directly to the virtual disk drive, which in this case is provided by the hypervisor. 4\. The block is passed by the PV disk driver not to an actual disk drive but to the hypervisor's (HV) virtual block driver (VBD), which is emulating a disk drive for the guest OS. 

  * At this point we have passed from the "virtualization" layer into the "physical" or real world.
5\. Repeating the process for the hypervisor OS, we now write the block to the filesystem (FS) or possibly volume manager (VM), depending on how your virtualization was configured. 6\. Again, the block stack handles requests to write the block. 7\. A disk driver (DD) writes the data to an actual disk. 8\. The block is passed to a RAID controller to be written. Obviously, laid out like this, the entire task already seems somewhat complex, but this is modern computing with many layers and abstractions. Even with DAS, if something went wrong, there are many places we might need to look to troubleshoot the issue — roughly eight, though the steps I listed are an approximation for the purpose of this article. SAN increases the complexity significantly, where starting at step 7, we take a completely different route: 9\. Instead of writing to the disk driver (DD), in a SAN system we write a network-based block device: the "iSCSI stack," [3] which provides SCSI [4] commands over TCP/IP. 10\. The iSCSI stack then sends the block to the hypervisor's TCP/IP stack to send it over the network. 11\. The block is now sent "across the wire," itself a somewhat complicated process that might involve things like packet fragmentation and MTU issues, TCP window size issues/adjustments, and more — all thankfully left out here in order to keep things simple. 12\. Now, at the SAN storage system, the TCP/IP stack receives the block of data. 13\. The block is handed off to the iSCSI stack for processing. 14\. The SAN filesystem and/or volume manager determines where to write the data. 15\. The block is passed to the block stack. 16\. The block is passed to the disk driver to write out. 17\. The hardware RAID writes the actual data to disk. In all, adding the SAN conservatively doubles the number of steps and moving parts involved. Each step or piece of software may be a cause of failure. Problems with tuning or performance may arise because of interactions between any two components. Problems with one component may spark issues with another. To complicate matters, troubleshooting the issues may be complex, in that the guest OS might be Windows, the hypervisor could be Linux, and the SAN might be some other OS all together. All with different filesystems, block stacks, iSCSI software, and TCP/IP stacks. **Complexity isn't linear** The problem, however, is not so much that there are more pieces, but that those pieces all potentially interact with each other and can cause issues. The problem is multiplicative. There are twice as many parts (or more) in a SAN, but that creates four times as many potential interactions, each of which could be a failure. The following figure shows all the steps as both rows and columns. Interactions could theoretically occur between each two of the steps. (I've blacked out the squares where a component intersects with itself because that's not an interaction between different components.) ![Complexity matrix](http://cdn.oreilly.com/radar/images/posts/0412-simple-vs-complex-580.png) _[Click to enlarge](http://cdn.oreilly.com/radar/images/posts/0412-simple-vs-complex-lg.png)._

## Comments

**[Chuck Hollis](#3368 "2012-06-14 13:41:00"):** Hi Randy I agree with your points -- up to a point.  All things being equal, more simplicity is better than more complexity.  But I think you end up missing a key aspect. To start with a non-IT analogy, modern aircraft are wonderfully complex, not to mention incredibly reliable.  Based on this blog post, none of us should ever fly again!  So what's the difference? As we both know, IT architectures are abstracted and layered.  Complexity is hidden, redundancy built in at each layer.  I make a file system call, it returns a status.  How it got done (or what machinations it had to do to recover from CRC errors, protocol failures, etc.) should be none of my concern. And, based on what I've seen in the architectures you've worked on, the same principles are in play. So how do you reconcile this school of thought? \-- Chuck

**[randybias](#3369 "2012-06-14 14:32:00"):** Hi Chuck, Always great to hear from you. Modern aircraft are complex and reliable; however, if compared to a less complex flight technology (e.g. hang-glider), they still have higher failure rates. I would also suggest that modern aircraft are better at handling failure modes than IT. As you and EMC well know, the trend towards scale-out systems, as epitomized by the larger Internet players has shown that more smaller, cheaper, and less reliable systems (i.e. "simpler") can be wired together into a bigger system that is inherently more reliable. A lot of this is a movement of the complexity from one layer to another, not the hiding of the complexity in each layer. More complex systems are typically made up of lots of smaller, simpler components. Systems with lots of complex components don't tend to get very large. What is Hadoop, or Isilon, for that matter, other than an abstraction at the app layer that reduces the need for complexity at the storage layer? RAID is no longer required, nor is server or storage redundancy. Instead it's moved upwards and baked into the application. Does that answer your question? \--Randy

**[Geoff Arnold](#3370 "2012-06-14 16:43:00"):** There are two fundamental problems with this analysis. The first is that "failure" is not really defined; for a storage service there are a variety of failure modes (silent data corruption, data loss, etc.), and the probability of each kind of failure is different. Moreover the notion of failure needs to be put into an appropriate systems context: an individual server with a local disk may not itself fail, but if the top-of-rack switch breaks, the data is inaccessible to the rest of the system, which may be counted as a systems failure.  The second issue is that the failure modes of the various components in your dependency chains are coupled in various ways. A server with a single power supply may have more components than a storage array with redundant power, but the single-point of failure affects multiple components. You correctly point out that the objective of much of our work in distributed systems has been to build reliable systems out of unreliable components.* However I fear that this kind of simplistic analysis obscures, rather than clarifies, our successes in this endeavor. \-- * This insight is due to my former CS Professor, Brian Randell at Newcastle-on-Tyne:

**[Dmitri Kalintsev](#3371 "2012-06-14 22:31:00"):** Hi Randy, Following your hand glider analogy - while it may be more reliable, it is at the same time isn't suitable for travel for many categories of passengers (most often, it happens, those who are willing to pay a lot to travel in comfort, even at a higher risk). Not everything can run on simple, not just yet. Things are moving there, but we're not quite there yet. Cheers, \-- Dmitri

**[Chuck Hollis](#3372 "2012-06-15 06:37:00"):** Still not entirely satisfied with your premise. Let's try another popular analogy -- the iPad: dead simple to use, very complex technology, supported by an even more complex ecosystem.  Also very reliable, based on my personal experience. Design and architecture makes it that way, not the parts count or how many layers there are in the technology.  Android, by comparison, exposes more of that complexity to the user. Going deeper, issues like scale, availability, complexity, etc. have to be dealt with somewhere -- whether that's buried in a subsystem or coded into an upper level application.   As one example, I've sat through a detailed explanation of how AWS provides availability services for their customers (and what their customers are responsible for) and I wouldn't use the word "simple" to describe it.  Ditto the deep dive into Facebook's architecture, and how they're thinking about availability.  Clearly, not "simple". The challenges are always the same; you just have choices about where you want to hide the nastiness. \-- Chuck

**[randybias](#3373 "2012-06-15 10:14:00"):** Chuck,   The iPad is dead simple to use because of Apple's focus on removing redundant UI elements and ruthless removal of unnecessary or underused features.  The opposite of the approach that Microsoft takes with Word.   As to the iPad's reliability, much of this is because it has less moving parts, just as I asserted in this article.  In fact, Apple focused on creating a SOC (system-on-a-chip) that integrated processing, graphics, I/O, etc.  It reminds me of the early 3com NIC cards in the early 90s.  Remember those?  Using jumpers to change IRQ settings?  All eventually replaced by single chip ASIC-based NICs that were software programmable.  Again, this is mostly simplification.  A single chip system is less likely to fail that a multi-chip system.   I said this in the article and I've said it elsewhere: all systems tends towards complexity.  The more cynical might even say that complexity is a form of entropy.  Well-built, scalable systems attempt to reduce the number of component parts, keep individual parts as simple as possible, in an effort to reduce the overall system complexity as a system grows.   My article isn't saying that nothing should ever be complex.  It's saying that complexity kills scalability.  Period.  I don't even think that's debatable.  And that achieving larger, more complex systems, that scale, requires individual parts to be simpler.   I think your points re: AWS and Facebook are irrelevant.  If you look at their individual components and approach, they strive for simplicity at the component layer and they strive to have less components.  The problem is that at sufficient size, complexity arises NO MATTER WHAT.  The core issue this article brings to light is that you can't make a system scale without this focus on having individual components be simpler. Going back to your original airline analogy, I don't think that it's appropriate. As Dmitri points out in his reply, a hang glider may have a better failure rate than a more complex modern aircraft, however, it has less capability. Complex systems can and do exist. They must in order for us to deal with more complex applications. Their is a considerable difference between IT and aerospace however. For the Boeing 787 Dreamliner, it took nearly 8 years, 5,000 test flight hours, and 2,000 test flights, before being delivered to production. Very few IT systems face this kind of rigor. The few that due, like filesystems, have similar failure profiles. More complex IT systems can and are created; however, the reality of the matter is that given the speed of innovation in the IT space, putting 8 years of testing into any complex system that would be put into production just isn't feasible.

**[randybias](#3374 "2012-06-15 10:34:00"):** Geoff, Good to hear from you. Any article that covers this topic is bound to be a simplification. These are deep issues that aren't easily covered in short sound bites. A large part of this article is intended to highlight the challenges in assuming that five 5-9 components != 5-9s of uptime. I appreciate your input on defining failure modes and I agree. You are quite right. Covering this in the detail necessary for it to make sense and factoring it into failure calculations is more of a masters type thesis in a compsci program. I'm not sure that trying to cover this in detail in a blog posting makes sense; however, a follow up explaining the variety of failure modes would be very appropriate. I would be happy to work on such a posting and even invite you to contribute if you are interested. Regarding your second issue, again, this is an area where I simplified, although I think your particular example doesn't really help as any storage array is connected to a 'head' of some kind that is equivalent to a server or a server in fact. Also, while a storage array may be simpler and dependent on the more complex head, I've seen storage arrays fail en toto due to things like a loose FC cable between storage array systems. As you say, there is dependency here, all of which highlights the issue in looking at uptime as a simple equation. I disagree that distributed systems work has been focused on making unreliable components reliable. I think that's a side effect of the work and perhaps, now with the guidance from practical experience derived from larger scale systems like Google and Amazon it's seen as a de facto part of distributed systems work, but I don't think the focus was originally on unreliable components so much as on how to build larger systems that were dependent on the network for an interconnection fabric. Basically, the initial move away from single monolithic big iron boxes in the early 80s drove much of this as you know. I still find myself talking to many in the market who believe that bigger, more complex, components, with more redundancy will lead to more successful large scale systems, despite the fact that so far this hasn't held true. I still also folks overly focused on the uptime statistics of individual components rather than the system as a whole. Too much thinking: "If I buy all 99.999% uptime systems at a 10x price premium I'll have a 99.999% uptime service." Of course, this isn't true. This article helps to further the discussion around how to build larger systems in a meaningful way that is understandable to less technical folks than yourself. We have to walk before we can run and not enough discussion is being up-leveled such that business owners and less technical folks than yourself can have a better understanding of core philosophies that can lead to successful large scale deployments. Best, \--Randy

**[RootWyrm](#3375 "2012-06-18 15:39:00"):** Unfortunately, this comparison is both false and completely misses the point of DAS vs. NAS vs. SAN. First of all, it pretends that distributed filesystems are not complex pieces of software. The opposite is true - they are much, much more complex than any RAID implementation in the past 20+ years and probably next 20 on top of it. If distributed filesystems were somehow at all easy, everybody and their brother would have written one back in the 90's. Secondly, it pretends that distributed filesystems somehow are not analogous to NAS, when they very much are. In fact, you double down on complexity with distributed as compared to NAS, because you're adding a second filesystem stack in between the TCP/IP stack and the OS stack. On any modern sane OS, that means you've added four to five layers - DFS, Network, OS Filesystem, OS Disk Abstraction are added to the chain. The REAL chain of a DFS looks like THIS in fact: Network - Application - DFS - Network (IP - TCP - UDP) - Ethernet Driver - Switch - Ethernet Driver - OS Filesystem - OS Disk Abstraction - OS Disk Driver - Hardware RAID - SATA PHY - SATA Cache Algorithm - Platter That's a total of 17 layers between your action and your data being committed. (Trust me, it looks better when I whiteboard this.) That excludes the DFS stack which is a duplication of EVERYTHING between DFS and OS Disk Driver since operations must be repeated across multiple distinct hardware systems. If we presume a 60% performance penalty due to replication, that means your network throughput availability on GigE drops to ~47.5MB/s after various adjustments. And that's best case, I'll add. (Base 125MB/s, adjust to 95% for combined bus/driver/buffer/checksum losses on PCIe.) But more importantly, it ignores the point of having NAS/SAN, which is data availability. Pretending that DAS offers equivalent or even comparable data availability, and/or that DAS offers greater reliability is just absurd. Facts are facts, and some ugly facts about DAS. One, disks are rarely portable - hardware goes down, data access is lost, period - especially with in-chassis RAID systems. Two, to achieve even the equivalent of redundant controllers requires two fully built identical DAS systems - and that only gets you to redundant controllers. Three, to achieve equivalent availability of a typical SAN requires doubling all hardware - that's DAS systems doubled, Ethernet ports per system doubled, Ethernet switches doubled. Oh, and if your DFS should happen to break - which is not an unknown thing by any stretch of the imagination - it's even deader than a faulted SAN controller. (And based on experience, you're likely to have many, many more corrupted blocks, too.) But more importantly, both NAS and SAN offer superior availability and mobility to DAS. If I want to move an application and it's data from a machine with exhausted DRAM resources, DAS means I have to wait hours to copy that data, I need to have disk space available on the target, and when it's done I have a machine with a ton of free space sitting there spinning idle. Either that or I've got a hundred thousand lines of potential breakage and <50% throughput availability best case w/increased latency. In a well engineered NAS/SAN scenario, the facts are quite different. I don't need to copy that data anywhere. All I need to do is unmount the associated LUNs from the exhausted system and remount them on the target system. Obviously presumes port-zoning, which I'd call best practice for any high mobility scenario. Presumably I also have a hot replica site; with a few notable (and highly publicized) exceptions, modern arrays will alarm and isolate between sites. Not to mention you now need to manage each DAS box individually - which is infinitely more complex, no matter how automated you make it. There's a term for that kind of silliness - Disk Storage Jenga (or Disk Storage Tetris.) Or you have a DFS, which is incredibly complex software which requires the SAME amount of additional hardware AND protocols AND is far more prone to breakage.

**[randybias](#3376 "2012-06-18 15:43:00"):** What has DFS got to do with anything? I didn't make a case for distributed filesystems anywhere in this article. DFS is as or more complex than SAN or NAS. Really scratching my head trying to figure out how you made that huge leap.

**[randybias](#3377 "2012-06-18 16:29:00"):** I'm really bummed that I failed to communicate these ideas and concepts to you as well as I would like.  This article is about building simpler systems and uses storage as an example.  Individual DAS systems that do not require replication or data redundancy at the infrastructure level are inherently simpler and easier to manage.  Period. You have assumed that data replication and redundancy is required at the infrastructure level, yet newer cloud-ready applications actually manage the data replication and redundancy at the levels above.  In fact, many new cloud technologies such as Hadoop specifically avoid even RAID since they add a layer of complexity.  Since Hadoop detects server and disk failures and does it's own data replication, RAID and data redundancy/replication at the infrastructure layer are completely useless. Here's my presentation from the OpenStorage Summit that talks about next generation storage in more detail:   http://www.youtube.com/watch?v=xcz2E5ba-fw You may not agree, which is fine, but my points are still valid: 1)  DAS is simpler.  Period. 2)  SAN/NAS add unnecessary complexity in some situations I agree with all of your points, but they are ONLY valid when you have applications that rely on the infrastructure to provide data redundancy and replication.  They are invalid in new emerging elastic cloud infrastructures where the application is largely responsible for these.  And in those cases, running DAS on the underlying compute nodes is easier, cleaner, scales better, has less failure points, and is much, much easier. And that's why it's Amazon Web Services default storage model.

**[Unjigalimonsito](#3384 "2012-07-27 02:53:00"):** This simplicity VS complexity topic reminds of the RISC vs CISC debate decades ago. Then finally Intel added a "RISC interpreter" to their processors (It may look CISC on the outside but its actually RISC underneath). So I guess I agree with you. Simplicity implies fast execution and a scalable architecture.

